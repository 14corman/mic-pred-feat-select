{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# K-Mer research with XGBoost\n",
    "Using K-Mer counts as input for XGBoost algorithm to predict MICs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Install dependencies\r\n",
    "import sys\r\n",
    "\r\n",
    "!{sys.executable} -m pip install seaborn\r\n",
    "!{sys.executable} -m pip install sagemaker==2.46.0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from seaborn) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.21.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (57.4.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting sagemaker==2.46.0\n",
      "  Using cached sagemaker-2.46.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (1.21.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (0.2.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (19.3.0)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (1.18.2)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (3.17.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (20.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker==2.46.0) (0.2.8)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.2 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker==2.46.0) (1.21.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker==2.46.0) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker==2.46.0) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.22.0,>=1.21.2->boto3>=1.16.32->sagemaker==2.46.0) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.22.0,>=1.21.2->boto3>=1.16.32->sagemaker==2.46.0) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.46.0) (2.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker==2.46.0) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker==2.46.0) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker==2.46.0) (2019.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker==2.46.0) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker==2.46.0) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker==2.46.0) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker==2.46.0) (1.6.6.4)\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.49.1\n",
      "    Uninstalling sagemaker-2.49.1:\n",
      "      Successfully uninstalled sagemaker-2.49.1\n",
      "Successfully installed sagemaker-2.46.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sagemaker\r\n",
    "import boto3\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "matplotlib.style.use(\"ggplot\")\r\n",
    "\r\n",
    "# Region to get estimator container\r\n",
    "region = boto3.Session().region_name\r\n",
    "\r\n",
    "# IAM role for executing the processing job.\r\n",
    "iam_role = sagemaker.get_execution_role()\r\n",
    "\r\n",
    "# Instance count to train on\r\n",
    "instance_count = 5\r\n",
    "\r\n",
    "# Type of EC2 instance to train on \r\n",
    "# (It is recommended to use m5 types rather than c5 types due to m5 having more memory since Machine Learning can be more Memory than CPU bound)\r\n",
    "# (The training data is 56GB, so we need an instance that has at least that amount of memory to fit the whole dataset in memory at once)\r\n",
    "# (XGBoost requires that the whole training dataset is in memory to train)\r\n",
    "# (This instance has 64GB of memory)\r\n",
    "isntance_type = \"ml.m5.4xlarge\"\r\n",
    "\r\n",
    "# Type of EC2 isntance to deploy and test on\r\n",
    "# (This does not need as much memory since we will only be running 1 sample through at a time)\r\n",
    "deploy_instance_type = \"ml.t2.medium\"\r\n",
    "\r\n",
    "# Bucket that data and output should go\r\n",
    "bucket = \"BUCKET FOR TRAINING DATA\"\r\n",
    "prefix = \"PREFIX KEY FOR TRAINING DATA\"\r\n",
    "\r\n",
    "# Path to put all output files\r\n",
    "output_path = f\"{prefix}/output/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(f\"Role being used: {iam_role}\")\r\n",
    "print(f\"Region being used: {region}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Role being used: arn:aws:iam::076069858788:role/JMI-Sagemaker-Notebook-Service-Role\n",
      "Region being used: us-east-2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data\n",
    "The data should already be processed through the data flow job, and exported to the S3 bucket.\n",
    "We want to have the path and content type set up to pass in to the training input."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Set below variable to True if there is a new dataset and it will be loaded from S3, shuffle/split into training and validation, and reuploaded unzipped.\r\n",
    "setup_train_files = False\r\n",
    "if setup_train_files:\r\n",
    "    print(\"Downloading zip file...\")\r\n",
    "    boto3.resource(\"s3\").Bucket(bucket).download_file(f\"{prefix}/train.zip\", \"train.zip\")\r\n",
    "    print(\"zip file downloaded\")\r\n",
    "    \r\n",
    "    print(\"Unzipping file...\")\r\n",
    "    !unzip train.zip\r\n",
    "    print(\"file unzipped\")\r\n",
    "    \r\n",
    "    print(\"Splitting into 5 files\")\r\n",
    "    # Found from: https://stackoverflow.com/a/20622193/9659107 (combined with a comment from that post)\r\n",
    "    !split -l$((`wc -l < train.libsvm`/5)) train.libsvm train- --verbose -da 1 --additional-suffix=\".libsvm\"\r\n",
    "    print(\"Split done\")\r\n",
    "    \r\n",
    "    print(\"Uploading files to S3\")\r\n",
    "    for i in range(5):\r\n",
    "        print(f\"Uploading training file {i}\")\r\n",
    "        boto3.client(\"s3\").upload_file(f\"train-{i}.libsvm\", bucket, f\"{prefix}/train-{i}.libsvm\")\r\n",
    "        print(f\"Training file {i} uploaded\")\r\n",
    "        \r\n",
    "    print(\"Training file split and upload successfully done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "processed_training_input_s3_path = f\"s3://{bucket}/{prefix}/train-\"\r\n",
    "validation_input_s3_path = f\"s3://{bucket}/{prefix}/validation.libsvm\"\r\n",
    "train_content_type = \"libsvm\"\r\n",
    "\r\n",
    "\r\n",
    "train_input = sagemaker.inputs.TrainingInput(\r\n",
    "    s3_data=processed_training_input_s3_path,\r\n",
    "    s3_data_type=\"S3Prefix\",\r\n",
    "    content_type=train_content_type,\r\n",
    "    distribution=\"ShardedByS3Key\"\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "validation_input = sagemaker.inputs.TrainingInput(\r\n",
    "    s3_data=validation_input_s3_path,\r\n",
    "    content_type=train_content_type\r\n",
    ")\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Estimator and Training\n",
    "Now we create the K-Means estimator and train it using the data and container from above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"latest\")\r\n",
    "hyperparameters = {\r\n",
    "    \"max_depth\":\"8\",\r\n",
    "    \"eta\":\"0.125\",\r\n",
    "    \"subsample\":\"1\",\r\n",
    "    \"objective\":\"reg:linear\",\r\n",
    "    \"num_round\":\"50\",\r\n",
    "    \"tree_method\": \"approx\"\r\n",
    "}\r\n",
    "\r\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \r\n",
    "                                          hyperparameters=hyperparameters,\r\n",
    "                                          role=iam_role,\r\n",
    "                                          instance_count=instance_count, \r\n",
    "                                          instance_type=isntance_type, \r\n",
    "                                          volume_size=64, # 64 GB (Needs to be larger than dataset since dataset will be stored in volume)\r\n",
    "                                          output_path=f\"s3://{bucket}/{output_path}\",\r\n",
    "                                          base_job_name=f\"{prefix}\"\r\n",
    "                                         )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "%%time\r\n",
    "estimator.fit({'train': train_input, \"validation\": validation_input})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-07-27 12:01:48 Starting - Starting the training job...\n",
      "2021-07-27 12:01:50 Starting - Launching requested ML instancesProfilerReport-1627387308: InProgress\n",
      "...\n",
      "2021-07-27 12:02:40 Starting - Preparing the instances for training.........\n",
      "2021-07-27 12:04:14 Downloading - Downloading input data..........................................\n",
      "2021-07-27 12:11:17 Training - Downloading the training image..\u001b[32mArguments: train\u001b[0m\n",
      "\u001b[33mArguments: train\u001b[0m\n",
      "\u001b[33m[2021-07-27:12:11:32:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:32:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[32m[2021-07-27:12:11:32:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:36:INFO] Number of hosts: 5, master IP address: 10.0.222.248, host IP address: 10.0.222.248.\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:36:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[36mArguments: train\u001b[0m\n",
      "\u001b[36m[2021-07-27:12:11:33:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[36m[2021-07-27:12:11:36:INFO] Number of hosts: 5, master IP address: 10.0.222.248, host IP address: 10.0.212.171.\u001b[0m\n",
      "\u001b[36m[2021-07-27:12:11:36:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[36mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-0-212-171.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[33m[2021-07-27:12:11:36:INFO] Number of hosts: 5, master IP address: 10.0.222.248, host IP address: 10.0.247.76.\u001b[0m\n",
      "\u001b[33m[2021-07-27:12:11:36:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[33mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-0-247-76.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[35mArguments: train\u001b[0m\n",
      "\u001b[35m[2021-07-27:12:11:33:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[35m[2021-07-27:12:11:36:INFO] Number of hosts: 5, master IP address: 10.0.222.248, host IP address: 10.0.224.201.\u001b[0m\n",
      "\u001b[35m[2021-07-27:12:11:36:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-0-224-201.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[32m[2021-07-27:12:11:36:INFO] Number of hosts: 5, master IP address: 10.0.222.248, host IP address: 10.0.237.237.\u001b[0m\n",
      "\u001b[32m[2021-07-27:12:11:37:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[32mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-0-237-237.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[36mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-0-212-171.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[35mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-0-224-201.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[32mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-0-237-237.us-east-2.compute.internal.out\u001b[0m\n",
      "\n",
      "2021-07-27 12:11:36 Training - Training image download completed. Training in progress.\u001b[34mstarting namenode, logging to /opt/amazon/hadoop/logs/hadoop--namenode-ip-10-0-222-248.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[33mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-0-247-76.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[36m[2021-07-27:12:11:41:INFO] File size need to be processed in the node: 44574.06 mb. Available memory size in the node: {mem_size_str} mb\u001b[0m\n",
      "\u001b[33m[2021-07-27:12:11:41:INFO] File size need to be processed in the node: 44560.95 mb. Available memory size in the node: {mem_size_str} mb\u001b[0m\n",
      "\u001b[35m[2021-07-27:12:11:42:INFO] File size need to be processed in the node: 44568.82 mb. Available memory size in the node: {mem_size_str} mb\u001b[0m\n",
      "\u001b[34mstarting resourcemanager, logging to /opt/amazon/hadoop/logs/yarn--resourcemanager-ip-10-0-222-248.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[32m[2021-07-27:12:11:42:INFO] File size need to be processed in the node: 44603.79 mb. Available memory size in the node: {mem_size_str} mb\u001b[0m\n",
      "\u001b[34mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-0-222-248.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[34mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-0-222-248.us-east-2.compute.internal.out\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:48:INFO] File size need to be processed in the node: 44611.28 mb. Available memory size in the node: {mem_size_str} mb\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:48:INFO] HTTP server started....\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:48:INFO] Memory/core ratio is 7.77\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:48:INFO] Yarn setup: number of workers: 5, physical cores per worker: 8, physical memory per worker: 62.13g.\u001b[0m\n",
      "\u001b[34m[2021-07-27:12:11:48:INFO] Yarn job submitted successfully.\u001b[0m\n",
      "\u001b[34m2021-07-27 12:11:48,726 INFO start listen on 10.0.222.248:9091\u001b[0m\n",
      "\u001b[34m/xgboost/dmlc-core/tracker/dmlc_tracker/yarn.py:36: UserWarning: cannot find \"/xgboost/dmlc-core/tracker/dmlc_tracker/../yarn/dmlc-yarn.jar\", I will try to run build\n",
      "  warnings.warn(\"cannot find \\\"%s\\\", I will try to run build\" % YARN_JAR_PATH)\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:37: warning: Signal is internal proprietary API and may be removed in a future release\u001b[0m\n",
      "\u001b[34mimport sun.misc.Signal;\n",
      "               ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:38: warning: SignalHandler is internal proprietary API and may be removed in a future release\u001b[0m\n",
      "\u001b[34mimport sun.misc.SignalHandler;\n",
      "               ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:276: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal intSignal = new Signal(\"INT\");\n",
      "        ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:276: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal intSignal = new Signal(\"INT\");\n",
      "                               ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:277: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal.handle(intSignal, handler);\n",
      "        ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:332: warning: SignalHandler is internal proprietary API and may be removed in a future release\n",
      "    class CtrlCHandler implements SignalHandler{\n",
      "                                  ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:339: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        public void handle(Signal signal){\n",
      "                           ^\u001b[0m\n",
      "\u001b[34mNote: src/main/java/org/apache/hadoop/yarn/dmlc/ApplicationMaster.java uses unchecked or unsafe operations.\u001b[0m\n",
      "\u001b[34mNote: Recompile with -Xlint:unchecked for details.\u001b[0m\n",
      "\u001b[34m7 warnings\u001b[0m\n",
      "\u001b[34m21/07/27 12:11:51 INFO client.RMProxy: Connecting to ResourceManager at algo-1/10.0.222.248:8032\u001b[0m\n",
      "\u001b[34m21/07/27 12:11:51 INFO dmlc.Client: HDFS temp directory do not exist, creating.. /tmp\u001b[0m\n",
      "\u001b[34m21/07/27 12:11:52 INFO dmlc.Client: jobname=DMLC[nworker=5]:python3,username=root\u001b[0m\n",
      "\u001b[34m21/07/27 12:11:52 INFO dmlc.Client: Submitting application application_1627387903437_0001\u001b[0m\n",
      "\u001b[34m21/07/27 12:11:52 INFO impl.YarnClientImpl: Submitted application application_1627387903437_0001\u001b[0m\n",
      "\u001b[34m2021-07-27 12:11:59,110 INFO @tracker All of 5 nodes getting started\u001b[0m\n",
      "\u001b[34m2021-07-27 12:14:38,824 INFO [0]#011train-rmse:174.043#011validation-rmse:13.3253\u001b[0m\n",
      "\u001b[34m2021-07-27 12:15:26,100 INFO [1]#011train-rmse:167.479#011validation-rmse:11.7153\u001b[0m\n",
      "\u001b[34m2021-07-27 12:16:10,923 INFO [2]#011train-rmse:161.767#011validation-rmse:10.3063\u001b[0m\n",
      "\u001b[34m2021-07-27 12:16:58,587 INFO [3]#011train-rmse:156.814#011validation-rmse:9.08185\u001b[0m\n",
      "\u001b[34m2021-07-27 12:17:44,548 INFO [4]#011train-rmse:152.534#011validation-rmse:8.003\u001b[0m\n",
      "\u001b[34m2021-07-27 12:18:32,155 INFO [5]#011train-rmse:148.847#011validation-rmse:7.07232\u001b[0m\n",
      "\u001b[34m2021-07-27 12:19:09,344 INFO [6]#011train-rmse:145.68#011validation-rmse:6.25712\u001b[0m\n",
      "\u001b[34m2021-07-27 12:19:47,339 INFO [7]#011train-rmse:142.968#011validation-rmse:5.55483\u001b[0m\n",
      "\u001b[34m2021-07-27 12:20:32,737 INFO [8]#011train-rmse:140.65#011validation-rmse:4.93836\u001b[0m\n",
      "\u001b[34m2021-07-27 12:21:14,220 INFO [9]#011train-rmse:138.674#011validation-rmse:4.40493\u001b[0m\n",
      "\u001b[34m2021-07-27 12:22:07,115 INFO [10]#011train-rmse:136.993#011validation-rmse:3.94338\u001b[0m\n",
      "\u001b[34m2021-07-27 12:22:49,184 INFO [11]#011train-rmse:135.566#011validation-rmse:3.55411\u001b[0m\n",
      "\u001b[34m2021-07-27 12:23:38,564 INFO [12]#011train-rmse:134.355#011validation-rmse:3.213\u001b[0m\n",
      "\u001b[34m2021-07-27 12:24:22,472 INFO [13]#011train-rmse:133.33#011validation-rmse:2.92787\u001b[0m\n",
      "\u001b[34m2021-07-27 12:25:15,644 INFO [14]#011train-rmse:132.463#011validation-rmse:2.68185\u001b[0m\n",
      "\u001b[34m2021-07-27 12:26:08,239 INFO [15]#011train-rmse:131.73#011validation-rmse:2.47257\u001b[0m\n",
      "\u001b[34m2021-07-27 12:26:58,020 INFO [16]#011train-rmse:131.112#011validation-rmse:2.29806\u001b[0m\n",
      "\u001b[34m2021-07-27 12:28:00,196 INFO [17]#011train-rmse:130.59#011validation-rmse:2.15547\u001b[0m\n",
      "\u001b[34m2021-07-27 12:28:48,690 INFO [18]#011train-rmse:130.15#011validation-rmse:2.02955\u001b[0m\n",
      "\u001b[34m2021-07-27 12:29:37,947 INFO [19]#011train-rmse:129.779#011validation-rmse:1.93334\u001b[0m\n",
      "\u001b[34m2021-07-27 12:30:43,620 INFO [20]#011train-rmse:129.467#011validation-rmse:1.85226\u001b[0m\n",
      "\u001b[34m2021-07-27 12:31:36,798 INFO [21]#011train-rmse:129.204#011validation-rmse:1.78832\u001b[0m\n",
      "\u001b[34m2021-07-27 12:32:28,442 INFO [22]#011train-rmse:128.982#011validation-rmse:1.73467\u001b[0m\n",
      "\u001b[34m2021-07-27 12:33:30,366 INFO [23]#011train-rmse:128.796#011validation-rmse:1.69413\u001b[0m\n",
      "\u001b[34m2021-07-27 12:34:24,808 INFO [24]#011train-rmse:128.64#011validation-rmse:1.65216\u001b[0m\n",
      "\u001b[34m2021-07-27 12:35:17,836 INFO [25]#011train-rmse:128.508#011validation-rmse:1.62687\u001b[0m\n",
      "\u001b[34m2021-07-27 12:36:09,932 INFO [26]#011train-rmse:128.397#011validation-rmse:1.6005\u001b[0m\n",
      "\u001b[34m2021-07-27 12:37:02,332 INFO [27]#011train-rmse:128.304#011validation-rmse:1.58034\u001b[0m\n",
      "\u001b[34m2021-07-27 12:38:00,774 INFO [28]#011train-rmse:128.225#011validation-rmse:1.56425\u001b[0m\n",
      "\u001b[34m2021-07-27 12:38:53,071 INFO [29]#011train-rmse:128.16#011validation-rmse:1.55091\u001b[0m\n",
      "\u001b[34m2021-07-27 12:39:51,167 INFO [30]#011train-rmse:128.104#011validation-rmse:1.54152\u001b[0m\n",
      "\u001b[34m2021-07-27 12:40:49,596 INFO [31]#011train-rmse:128.058#011validation-rmse:1.53169\u001b[0m\n",
      "\u001b[34m2021-07-27 12:41:39,123 INFO [32]#011train-rmse:128.018#011validation-rmse:1.52319\u001b[0m\n",
      "\u001b[34m2021-07-27 12:42:40,688 INFO [33]#011train-rmse:127.985#011validation-rmse:1.51652\u001b[0m\n",
      "\u001b[34m2021-07-27 12:43:39,913 INFO [34]#011train-rmse:127.958#011validation-rmse:1.5095\u001b[0m\n",
      "\u001b[34m2021-07-27 12:44:35,367 INFO [35]#011train-rmse:127.935#011validation-rmse:1.50162\u001b[0m\n",
      "\u001b[34m2021-07-27 12:45:40,749 INFO [36]#011train-rmse:127.915#011validation-rmse:1.49802\u001b[0m\n",
      "\u001b[34m2021-07-27 12:46:39,274 INFO [37]#011train-rmse:127.899#011validation-rmse:1.49576\u001b[0m\n",
      "\u001b[34m2021-07-27 12:47:39,780 INFO [38]#011train-rmse:127.885#011validation-rmse:1.49286\u001b[0m\n",
      "\u001b[34m2021-07-27 12:48:40,060 INFO [39]#011train-rmse:127.873#011validation-rmse:1.49184\u001b[0m\n",
      "\u001b[34m2021-07-27 12:49:37,548 INFO [40]#011train-rmse:127.863#011validation-rmse:1.48927\u001b[0m\n",
      "\u001b[34m2021-07-27 12:50:44,536 INFO [41]#011train-rmse:127.855#011validation-rmse:1.48561\u001b[0m\n",
      "\u001b[34m2021-07-27 12:51:39,042 INFO [42]#011train-rmse:127.848#011validation-rmse:1.48556\u001b[0m\n",
      "\u001b[34m2021-07-27 12:52:38,894 INFO [43]#011train-rmse:127.842#011validation-rmse:1.48505\u001b[0m\n",
      "\u001b[34m2021-07-27 12:53:14,379 INFO [44]#011train-rmse:127.838#011validation-rmse:1.48465\u001b[0m\n",
      "\u001b[34m2021-07-27 12:54:11,458 INFO [45]#011train-rmse:127.833#011validation-rmse:1.48303\u001b[0m\n",
      "\u001b[34m2021-07-27 12:54:46,703 INFO [46]#011train-rmse:127.83#011validation-rmse:1.48241\u001b[0m\n",
      "\u001b[34m2021-07-27 12:55:32,186 INFO [47]#011train-rmse:127.827#011validation-rmse:1.4822\u001b[0m\n",
      "\u001b[34m2021-07-27 12:56:21,839 INFO [48]#011train-rmse:127.825#011validation-rmse:1.48213\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:06,206 INFO [49]#011train-rmse:127.823#011validation-rmse:1.48261\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:07,488 INFO Finished training\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:07,506 INFO Finished training\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:07,533 INFO Finished training\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:07,558 INFO Finished training\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:07,618 INFO Finished training\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:07,627 INFO @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34m2021-07-27 12:57:07,628 INFO @tracker 2708.5175547599792 secs between node start and job finish\u001b[0m\n",
      "\u001b[33m[2021-07-27:12:57:44:INFO] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\u001b[36m[2021-07-27:12:58:49:INFO] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\u001b[32m[2021-07-27:12:58:49:INFO] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\u001b[35m[2021-07-27:12:58:50:INFO] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "\n",
      "2021-07-27 12:59:06 Uploading - Uploading generated training model\n",
      "2021-07-27 12:59:06 Completed - Training job completed\n",
      "ProfilerReport-1627387308: NoIssuesFound\n",
      "Training seconds: 16540\n",
      "Billable seconds: 16540\n",
      "CPU times: user 11.5 s, sys: 613 ms, total: 12.2 s\n",
      "Wall time: 57min 31s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deploy\r\n",
    "Now that the model has been trained, we can deploy it to an instance. This is used to make it easier to look up the model later, and we can do testing against the endpoint if wanted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "%%time\r\n",
    "estimator_predictor = estimator.deploy(initial_instance_count=1, instance_type=deploy_instance_type)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------!CPU times: user 243 ms, sys: 16.8 ms, total: 260 ms\n",
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}